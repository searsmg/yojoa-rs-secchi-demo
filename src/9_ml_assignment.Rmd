---
title: "9_assignment"
author: "Matthew Ross, edited by Megan Sears"
date: "2023-04-10"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)

match_dir = 'data/matchups'
#model_dir = 'data/models/'

```

# Purpose

The original purpose of this script is to apply applying the `xgboost` algorithm 
to Remote Sensing Imagery of Lake Yojoa in Honduras, to estimate Yojoa water clarity. 
You can read more about this lake [here](https://www.sciencedirect.com/science/article/pii/S0048969722015479). We
have slightly adopted the code to become a teaching demo on how to use machine learning
algorithms. We also use a myriad of climate covariates 
from the ERA5 climate data in this analysis.

You can read more about xgboost all over the internet, but I like the 
kaggle [demo](https://www.kaggle.com/code/rtatman/machine-learning-with-xgboost-in-r/notebook)


# Data Prep and Demo

## Load matchup data

```{r}
#list all the files in the match directory
match = list.files(match_dir)

#load the three day matchup file. 
sameDay = read.csv(file.path(match_dir, match[grepl('same', match)]))
oneDay = read.csv(file.path(match_dir, match[grepl('one', match)]))
threeDay = read.csv(file.path(match_dir, match[grepl('three', match)]))
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match)]))


```

Prep the data for xgboost

```{r}
prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
  
  #Add ratios then trim to needd to columns to speed up run
  df_prep %>% 
    mutate(NR= med_Nir_corr/med_Red_corr,
           BG= med_Blue_corr/med_Green_corr,
           BR= med_Blue_corr/med_Red_corr)
}


sameDay <- prepData(sameDay)
oneDay <- prepData(oneDay)
threeDay <- prepData(threeDay)
fiveDay <- prepData(fiveDay)
```

We want to predict the `secchi` value in these datasets, so let's set the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```

## Quick xgboost run on fiveDay matchups

Let's see what happens if we loosen our time restraint and add more matchups into the mix

### Make test and training sets

For the same day matchup dataset, let's grab 20% of the data as the 'test' set and the remainder as the training set.

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
test_fiveDay <- fiveDay %>%
  sample_frac(.2) 

## Remove holdout data
train_fiveDay <- fiveDay %>% filter(!rowid %in% test_fiveDay$rowid) 

hist(train_fiveDay$secchi)
hist(test_fiveDay$secchi)
```


## Add in the met data with the five day matchups

Let's see what happens if we add in the ERA5 met data. For this example, we'll use the 5-day summaries, meaning we've summarized the met data as the mean of the previous 5 days. Since we already made the training/test datasets, let's stick with those, but name new features.

### xgboost on band data and all the 5-day met data

In our dataset, the 5-day met summaries have the suffix '\_5'

```{r}
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr', 'NR', 'BG', 'BR', 'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'min_temp_degK_5', 'tot_precip_m_5', 'mean_wind_mps_5')
```

Now we'll format the data

```{r}
## Format it the way xgboost likes
dtrain_fd_bm5 <- xgb.DMatrix(data = as.matrix(train_fiveDay[,band_met5_feats]), 
                      label = train_fiveDay[,target])
dtest_fd_bm5 <- xgb.DMatrix(data = as.matrix(test_fiveDay[,band_met5_feats]), 
                     label = test_fiveDay[,target])

```

And now train the model

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.05, 
               gamma=0.5, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5 <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Okay, the train and test are still pretty far apart (which means we're overfitted), but let's look at the data anyway

```{r}

preds_fd_bm5 <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5, dtest_fd_bm5))

evals_fd_bm5 <- preds_fd_bm5 %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

This generally looks better than before (our adjusted r-squared is higher that without the met data) AND it's better than the one-day matchups. Let's play around with the hyper parameters and see what happens...

### Try some different hyperparmeters

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.3, 
               gamma=2, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5_play <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Getting better... let's look at the data

```{r}

preds_fd_bm5_play <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_play, dtest_fd_bm5))

evals_fd_bm5_play <- preds_fd_bm5_play %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5_play, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```


# Assignment


## Hyperparameter tuning

For any machine learning pipeline, one of the things we can sink
the most time into is trying to tune our so called "hyperparameters" that alter how the ML algorithm "learns" from
the data. Hypertuning, can improve model performance substantially,
but it also can be a huge time-suck where model performance is
capped. Above you see some parameter differences between two
models and we have outputs where the second model performs better
than the first. 

Can you do better than the second model? Spend no more than 20 minutes
altering the hyperparameters and see if you can get a better RMSE AND R2. Here you should use `?xgb.train` to see what parameters are available and you can look at the code above to see which ones we altered. Please use the exact visualiztion code we use above (ggplot + r2 in top right). 

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.3, 
               gamma=2, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5_play2 <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)

preds_fd_bm5_play2 <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_play2, dtest_fd_bm5))

evals_fd_bm5_play2 <- preds_fd_bm5_play2 %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

# ggplot to check out 
ggplot(preds_fd_bm5_play2, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

```

#### Report your RMSE here



### What hypertuning parameters most impacted your model performance?

Here I'm just hoping you can describe the parameters that most impacted
model performance, can you provide an explanation why this parameters
mattered? 


## More Columns

We have kept this data fairly "narrow," not using too many columns for
prediction. However, there is a vast array of potential Spectral Indices one can potentially use to improve model performance. Using this [site](https://github.com/awesome-spectral-indices/awesome-spectral-indices), add
three new columns to our data with indices you think might help performance. Remember!
You will need to add these columns to the overall data BEFORE you split it into test/train and convert those to matrices for XGB boost. 

Some key words for spectral indices that might help include *chlorophyll*, *sediment*
*turbidity* and *clarity*. 

```{r}

```


### Reproduce the Validation Plot with New Indices

```{r}

```




### Does adding these three spectral indices to your dataset improve results? 


```{r}

```


## How does Random Forest Perform compared to XG Boost



Gradient boosting is only one of many possible "tree" based algorithms, the most
common form of these is an algorithm called "Random Forest." These two approaches
work similarly, but they have underlying differences. 

Can you build a new pipeline for the `randomForest` package? Using the same structure
as above? 
```{r}

```



# Bonus

What ways might we be "cheating" in our approach here? Hint: I may have made a 
mistake when splitting the data into train, test, validate. Hint 2: How does
estimating water quality at different sites on the same day impact performance? 


