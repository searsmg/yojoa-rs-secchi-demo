---
title: "9_assignment"
author: "Matthew Ross, edited by Megan Sears"
date: "2023-04-10"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)

match_dir = 'data/matchups'
#model_dir = 'data/models/'

```

# Purpose

The original purpose of this script is to apply applying the `xgboost` algorithm 
to Remote Sensing Imagery of Lake Yojoa in Honduras, to estimate Yojoa water clarity. 
You can read more about this lake [here](https://www.sciencedirect.com/science/article/pii/S0048969722015479). We
have slightly adopted the code to become a teaching demo on how to use machine learning
algorithms. We also use a myriad of climate covariates 
from the ERA5 climate data in this analysis.

You can read more about xgboost all over the internet, but I like the 
kaggle [demo](https://www.kaggle.com/code/rtatman/machine-learning-with-xgboost-in-r/notebook)


# Data Prep and Demo

## Load matchup data

```{r}
#list all the files in the match directory
match = list.files(match_dir)

#load the three day matchup file. 
sameDay = read.csv(file.path(match_dir, match[grepl('same', match)]))
oneDay = read.csv(file.path(match_dir, match[grepl('one', match)]))
threeDay = read.csv(file.path(match_dir, match[grepl('three', match)]))
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match)]))


```

Prep the data for xgboost

```{r}
prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
  
  #Add ratios then trim to needd to columns to speed up run
  df_prep %>% 
    mutate(NR= med_Nir_corr/med_Red_corr,
           BG= med_Blue_corr/med_Green_corr,
           BR= med_Blue_corr/med_Red_corr)
}


sameDay <- prepData(sameDay)
oneDay <- prepData(oneDay)
threeDay <- prepData(threeDay)
fiveDay <- prepData(fiveDay)
```

We want to predict the `secchi` value in these datasets, so let's set the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```

## Quick xgboost run on fiveDay matchups

Let's see what happens if we loosen our time restraint and add more matchups into the mix

### Make test and training sets

For the same day matchup dataset, let's grab 20% of the data as the 'test' set and the remainder as the training set.

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
test_fiveDay <- fiveDay %>%
  sample_frac(.2) 

## Remove holdout data
train_fiveDay <- fiveDay %>% filter(!rowid %in% test_fiveDay$rowid) 

hist(train_fiveDay$secchi)
hist(test_fiveDay$secchi)
```


## Add in the met data with the five day matchups

Let's see what happens if we add in the ERA5 met data. For this example, we'll use the 5-day summaries, meaning we've summarized the met data as the mean of the previous 5 days. Since we already made the training/test datasets, let's stick with those, but name new features.

### xgboost on band data and all the 5-day met data

In our dataset, the 5-day met summaries have the suffix '\_5'

```{r}
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 
                     'med_Nir_corr', 'NR', 'BG', 'BR', 'tot_sol_rad_KJpm2_5', 
                     'max_temp_degK_5', 'min_temp_degK_5', 'tot_precip_m_5', 'mean_wind_mps_5')
```

Now we'll format the data

```{r}
## Format it the way xgboost likes
dtrain_fd_bm5 <- xgb.DMatrix(data = as.matrix(train_fiveDay[,band_met5_feats]), 
                      label = train_fiveDay[,target])

dtest_fd_bm5 <- xgb.DMatrix(data = as.matrix(test_fiveDay[,band_met5_feats]), 
                     label = test_fiveDay[,target])

```

And now train the model

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.05, 
               gamma=0.5, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5 <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Okay, the train and test are still pretty far apart (which means we're overfitted), but let's look at the data anyway

```{r}

preds_fd_bm5 <- test_fiveDay %>% 
  mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5, dtest_fd_bm5))

evals_fd_bm5 <- preds_fd_bm5 %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

This generally looks better than before (our adjusted r-squared is higher that without the met data) AND it's better than the one-day matchups. Let's play around with the hyper parameters and see what happens...

### Try some different hyperparmeters

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.3, 
               gamma=2, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5_play <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Getting better... let's look at the data

```{r}

preds_fd_bm5_play <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_play, dtest_fd_bm5))

evals_fd_bm5_play <- preds_fd_bm5_play %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5_play, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```


# Assignment


## Hyperparameter tuning

For any machine learning pipeline, one of the things we can sink
the most time into is trying to tune our so called "hyperparameters" that alter how the ML algorithm "learns" from
the data. Hypertuning, can improve model performance substantially,
but it also can be a huge time-suck where model performance is
capped. Above you see some parameter differences between two
models and we have outputs where the second model performs better
than the first. 

Can you do better than the second model? Spend no more than 20 minutes
altering the hyperparameters and see if you can get a better RMSE AND R2. Here you should use `?xgb.train` to see what parameters are available and you can look at the code above to see which ones we altered. Please use the exact visualiztion code we use above (ggplot + r2 in top right). 

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.27, 
               gamma=1.8, 
               max_depth=3, 
               min_child_weight=3, 
               subsample=1, 
               colsample_bytree=1,
               lambda=0.99,
               alpha=0.05,
               num_parallel_tree=2)

#run the boost algo with those settings
xgb_naive_fd_bm5_play2 <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 200, 
                             maximize = F)

preds_fd_bm5_play2 <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_play2, dtest_fd_bm5))

evals_fd_bm5_play2 <- preds_fd_bm5_play2 %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

# ggplot to check out 
ggplot(preds_fd_bm5_play2, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))

```

#### Report your RMSE here

The validation RMSE is 0.71 m. 

### What hypertuning parameters most impacted your model performance?

The parameters that most impacted model performance were eta, gamma, max depth, and min child weight. Eta is the learning rate and the smaller the number (i.e., closer to 0), the slower the computation, but the better the result. In other words, a smaller eta makes the model more robust by shrinking the weights of each feature update (i.e., weight used to update each feature's importance). Gamma is the value that specifies the minimum reduction in the loss function that must be reached by splitting a node in the tree. Higher gamma values result in less splits since it requires a larger reduction in the loss function, which reduces the complexity of the model. The max depth limits the number of nodes in a tree by restricting the amount of splits. Since it is a smaller dataset, there is less to learn from and it does not need a large max depth. The min child weight controls the minimum amount of data required to make a split at a given node. Similar to max depth, with a smaller dataset, it may be best to use a smaller min child weight to avoid overfitting.

## More Columns

We have kept this data fairly "narrow," not using too many columns for
prediction. However, there is a vast array of potential Spectral Indices one can potentially use to improve model performance. Using this [site](https://github.com/awesome-spectral-indices/awesome-spectral-indices), add
three new columns to our data with indices you think might help performance. Remember!
You will need to add these columns to the overall data BEFORE you split it into test/train and convert those to matrices for XGB boost. 

Some key words for spectral indices that might help include *chlorophyll*, *sediment*
*turbidity* and *clarity*. 

```{r}
# (G - N) / (G + N) NDWI
# (N / G) - 1.0 	Chlorophyll Index Green
# (R-G)/(R+G) turbidity index

# add in the above band math
fiveDay_more <- fiveDay %>%
  mutate(ndwi = (med_Green_corr - med_Nir_corr) / (med_Green_corr + med_Nir_corr),
         chlor_g = (med_Nir_corr / med_Green_corr) - 1.0,
         turb = (med_Red_corr - med_Green_corr) / (med_Red_corr + med_Green_corr))

##Pull 20% as holdout test data
test_fiveDay_more <- fiveDay_more %>%
  sample_frac(.2) 

## Remove holdout data
train_fiveDay_more <- fiveDay_more %>% 
  filter(!rowid %in% test_fiveDay_more$rowid) 

hist(train_fiveDay_more$secchi)
hist(test_fiveDay_more$secchi)

# add in all these + new cols
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 
                     'med_Nir_corr', 'NR', 'BG', 'BR', 'tot_sol_rad_KJpm2_5', 
                     'max_temp_degK_5', 'min_temp_degK_5', 'tot_precip_m_5', 'mean_wind_mps_5',
                     'ndwi', 'chlor_g', 'turb')

## Format it the way xgboost likes
dtrain_fd_bm5_more <- xgb.DMatrix(data = as.matrix(train_fiveDay_more[,band_met5_feats]), 
                      label = train_fiveDay_more[,target])

dtest_fd_bm5_more <- xgb.DMatrix(data = as.matrix(test_fiveDay_more[,band_met5_feats]), 
                     label = test_fiveDay_more[,target])

#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.3, 
               gamma=2, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5_playmore <- xgb.train(params = params, 
                             data = dtrain_fd_bm5_more, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5_more, 
                                              val = dtest_fd_bm5_more), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)

preds_fd_bm5_playmore <- test_fiveDay_more %>% 
  mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_playmore, dtest_fd_bm5_more))

evals_fd_bm5_playmore <- preds_fd_bm5_playmore %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5))


```

### Reproduce the Validation Plot with New Indices

```{r}


ggplot(preds_fd_bm5_playmore, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))


```

### Does adding these three spectral indices to your dataset improve results? 

Yes, the r-squared value has increased to 0.69, and the validation RMSE is slightly smaller (0.69 m). 

## How does Random Forest Perform compared to XG Boost

Gradient boosting is only one of many possible "tree" based algorithms, the most
common form of these is an algorithm called "Random Forest." These two approaches
work similarly, but they have underlying differences. 

Can you build a new pipeline for the `randomForest` package? Using the same structure
as above? 

```{r}

library(randomForest)

rf_5_test <- train_fiveDay_more %>%
  select(all_of(band_met5_feats),secchi)

rf_5_mod <- randomForest(secchi ~ ., rf_5_test,
                         mtry = 2,
                         nodesize = 1,
                         ntree = 2000, #not a large dataset so quickly finding 
                         importance = T)
rf_5_mod
plot(rf_5_mod)

#apply this model to the test data
test_fiveDay_more$secchi_pred <- predict(rf_5_mod, test_fiveDay_more)

ggplot(test_fiveDay_more, aes(x=secchi, y=secchi_pred)) +
  geom_point() +
  geom_smooth(method = lm, color = "blue") +
  geom_abline(intercept = 0, slope = 1)
  
as_tibble(rf_5_mod$importance) %>%
  mutate(variable = row.names(rf_5_mod$importance)) %>% 
  arrange(`%IncMSE`)

Metrics::rmse(test_fiveDay$secchi, test_fiveDay$secchi_pred)

```

# Bonus

What ways might we be "cheating" in our approach here? Hint: I may have made a 
mistake when splitting the data into train, test, validate. Hint 2: How does
estimating water quality at different sites on the same day impact performance? 


